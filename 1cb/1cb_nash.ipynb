{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "db74ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6db08e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('decks.csv')\n",
    "matches = df.iloc[1:-2, 5:-2]\n",
    "utils = matches.astype(float).to_numpy()\n",
    "utils = utils - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ee616f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnad_replicator_step(x, y, M, pi_reg_row, pi_reg_col, eta=0.2, dt=0.02, eps=1e-12):\n",
    "    # Normalize inputs\n",
    "    x = np.clip(x, eps, None); x /= x.sum()\n",
    "    y = np.clip(y, eps, None); y /= y.sum()\n",
    "    pi_reg_row = np.clip(pi_reg_row, eps, None); pi_reg_row /= pi_reg_row.sum()\n",
    "    pi_reg_col = np.clip(pi_reg_col, eps, None); pi_reg_col /= pi_reg_col.sum()\n",
    "\n",
    "    # Payoffs (row maximizes, column minimizes)\n",
    "    q_row = M @ y                  # shape (n,)\n",
    "    q_col = - M.T @ x              # shape (m,)\n",
    "\n",
    "    # Regularized fitness\n",
    "    f_row = q_row - eta*(np.log(x + eps) - np.log(pi_reg_row + eps))\n",
    "    f_col = q_col - eta*(np.log(y + eps) - np.log(pi_reg_col + eps))\n",
    "\n",
    "    # Replicator RHS\n",
    "    u_row = f_row - x @ f_row\n",
    "    u_col = f_col - y @ f_col\n",
    "\n",
    "    # Multiplicative Euler step\n",
    "    x = x * np.exp(dt * u_row); x /= x.sum()\n",
    "    y = y * np.exp(dt * u_col); y /= y.sum()\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5a87382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:46<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Nash equilibrium support (row player):\n",
      "                      Deck  Weight\n",
      "   Chancellor of the Annex  0.2967\n",
      "      Koma, Cosmos Serpent  0.2147\n",
      "           Thassa's Oracle  0.1605\n",
      "           Absolute Virtue  0.1370\n",
      "Jace, Wielder of Mysteries  0.1363\n",
      "Herald of the eternal dawn  0.0549\n",
      "Entropy: 1.6876094914508124\n",
      "Converged in 63 outer loops: True\n",
      "Row sums: 0:12=0.297, 12:16=0.352, 16:=0.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_nashes = 1000\n",
    "nash_solutions = []\n",
    "\n",
    "threshold = 1e-8\n",
    "eta = 0.2\n",
    "max_iters = 1000 * 10  # safety cap\n",
    "M = utils\n",
    "\n",
    "# Get the deck names from the dataframe\n",
    "deck_names = df.iloc[1:-2, 1].reset_index(drop=True)  # assuming first column is name\n",
    "\n",
    "for i in tqdm(range(num_nashes)):\n",
    "    # init x, y uniform, but pi_reg with new random values\n",
    "    x = np.ones(M.shape[0]); x /= x.sum()\n",
    "    y = np.ones(M.shape[1]); y /= y.sum()\n",
    "    pi_reg_row = np.random.rand(M.shape[0]); pi_reg_row /= pi_reg_row.sum()\n",
    "    pi_reg_col = pi_reg_row.copy()\n",
    "\n",
    "    diff = np.inf\n",
    "    k = 0\n",
    "    while diff > threshold and k < max_iters:\n",
    "        for _ in range(1000):\n",
    "            x, y = rnad_replicator_step(x, y, M, pi_reg_row, pi_reg_col, eta=eta, dt=0.1)\n",
    "        prev_pi_reg_row = pi_reg_row.copy()\n",
    "        prev_pi_reg_col = pi_reg_col.copy()\n",
    "        pi_reg_row = x.copy()\n",
    "        pi_reg_col = y.copy()\n",
    "        diff = max(np.linalg.norm(pi_reg_row - prev_pi_reg_row), np.linalg.norm(pi_reg_col - prev_pi_reg_col))\n",
    "        k += 1\n",
    "\n",
    "    entropy_row = entropy(pi_reg_row)\n",
    "    nash_solutions.append({\n",
    "        \"row\": pi_reg_row.copy(),\n",
    "        \"col\": pi_reg_col.copy(),\n",
    "        \"entropy_row\": entropy_row,\n",
    "        \"iters\": k,\n",
    "        \"converged\": diff <= threshold\n",
    "    })\n",
    "\n",
    "# Pick the Nash equilibrium with the highest entropy in row player\n",
    "best_nash = max(nash_solutions, key=lambda v: v[\"entropy_row\"])\n",
    "\n",
    "# Prepare a nice output table for the support and its weight\n",
    "row_probs = best_nash[\"row\"]\n",
    "support_mask = row_probs > 1e-6\n",
    "support_names = deck_names[support_mask]\n",
    "support_weights = row_probs[support_mask]\n",
    "\n",
    "output_table = pd.DataFrame({\n",
    "    \"Deck\": support_names.values,\n",
    "    \"Weight\": support_weights\n",
    "}).sort_values(\"Weight\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Best Nash equilibrium support (row player):\")\n",
    "print(output_table.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "print(\"Entropy:\", best_nash[\"entropy_row\"])\n",
    "print(\"Converged in\", best_nash[\"iters\"], \"outer loops:\", best_nash[\"converged\"])\n",
    "print(\"Row sums: 0:12=%.3f, 12:16=%.3f, 16:=%0.3f\" % (\n",
    "    best_nash[\"row\"][0:12].sum(),\n",
    "    best_nash[\"row\"][12:16].sum(),\n",
    "    best_nash[\"row\"][16:].sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "620e1a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best response payoff is 6.169057055261138e-09\n",
      "Regret of best Nash equilibrium (row player): 6.1690570272841584e-09\n",
      "Is the row and column strategy the same? True\n"
     ]
    }
   ],
   "source": [
    "# Compute the expected payoff for the row player given best_nash\n",
    "row_strategy = best_nash[\"row\"]\n",
    "col_strategy = best_nash[\"col\"]\n",
    "expected_value = row_strategy @ M @ col_strategy\n",
    "\n",
    "# Compute the best response payoff for the row player\n",
    "best_response_payoff = np.max(M @ col_strategy)\n",
    "\n",
    "regret = best_response_payoff - expected_value\n",
    "\n",
    "print(\"The best response payoff is\", best_response_payoff)\n",
    "print(\"Regret of best Nash equilibrium (row player):\", regret)\n",
    "print(\"Is the row and column strategy the same?\", np.allclose(row_strategy, col_strategy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060793db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bagoftricks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
